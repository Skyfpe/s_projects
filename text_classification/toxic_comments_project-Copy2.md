# –ü—Ä–æ–µ–∫—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ —Å BERT

–ù–∞—à –∑–∞–∫–∞–∑—á–∏–∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é. 

**–ó–∞–¥–∞—á–∞:** –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –í –Ω–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫.

**–¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞:** *F1* –Ω–µ –º–µ–Ω—å—à–µ 0.75

**–ö—Ä–∞—Ç–∫–∏–π –ø–ª–∞–Ω –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞**

1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.
2. –ü—Ä–æ–≤–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.
3. –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ BERT.
4. –û–±—É—á–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
5. –í—ã–±—Ä–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å.
6. –°–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã.

**–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**

–î–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `toxic_comments.csv`. –°—Ç–æ–ª–±–µ—Ü *text* –≤ –Ω—ë–º —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –∞ *toxic* ‚Äî —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫.

<h1>–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞" data-toc-modified-id="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞</a></span></li><li><span><a href="#–û–±—É—á–µ–Ω–∏–µ" data-toc-modified-id="–û–±—É—á–µ–Ω–∏–µ-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>–û–±—É—á–µ–Ω–∏–µ</a></span></li><li><span><a href="#–í—ã–≤–æ–¥—ã" data-toc-modified-id="–í—ã–≤–æ–¥—ã-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>–í—ã–≤–æ–¥—ã</a>


```python
from IPython.display import display, HTML
display(HTML("<style>.container { width:70% !important; }</style>"))
```


<style>.container { width:70% !important; }</style>


## –ò–º–ø–æ—Ä—Ç—ã –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã


```python
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tqdm
from tqdm import notebook
import time

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

import transformers
import torch

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

RANDOM_STATE = 17
test_size = 0.25
```

## –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞

<img src="https://emojigraph.org/media/apple/check-mark-button_2705.png" align=left width=33, heigth=33>
<div class="alert alert-success"><font size="3">
    <b>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>
    
–£ –º–µ–Ω—è –æ—á–µ–Ω—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –≤—ã–Ω–æ—Å—É –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ –Ω–∞—á–∞–ª–æ. –° –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã —ç—Ç–æ –ª–æ–≥–∏—á–Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–æ–≥–∏–∫–µ —Ç–æ–≥–æ, –∫–∞–∫ python –∫–æ–¥ –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏  –Ω–æ—É—Ç–±—É–∫–æ–≤ (–ø—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–∞ python).
    
–° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã —ç—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —É —Ç–µ–±—è —Å–ø—Ä—è—Ç–∞–Ω–∞ –≤ —ç—Ç–æ–º  —Ä–∞–∑–¥–µ–ª–µ –∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≥–¥–µ-—Ç–æ –ø–æ —Å–µ—Ä–µ–¥–∏–Ω–µ.
    
–ü–æ—ç—Ç–æ–º—É –¥–ª—è —Å–µ–±—è —è –Ω–∞—à–µ–ª —Å–ª–µ–¥—É—é—â–µ–µ —Ä–µ—à–µ–Ω–∏–µ:
    
 - –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –Ω–µ—Å–µ—Ç –≤ —Å–µ–±–µ —á—Ç–æ-—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞, –æ–Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è, —Å–µ—Ä–≤–∏—Å–Ω–∞—è - —Ç–æ –¥–∞–≤–∞–π –≤—ã–Ω–µ—Å–µ–º –µ—ë –≤ –Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã, —á—Ç–æ–±—ã "–Ω–µ –º–µ—à–∞–ª–∞".
 - –∞ –≤–æ—Ç –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞, –Ω–µ—Å–µ—Ç –≤ —Å–µ–±–µ –≤–∞–∂–Ω—É—é –ª–æ–≥–∏–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–æ–∏—Å–∫ –ø–æ —Å–µ—Ç–∫–µ) —Ç–æ –ª—É—á—à–µ —Ä–∞—Å–ø–æ–ª–æ–∂–∏—Ç—å –µ—ë –±–ª–∏–∂–µ –∫ –∫–æ–¥—É, –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ.
    
    
    
 </div>

<img src="https://upload.wikimedia.org/wikipedia/commons/b/ba/Warning_sign_4.0.png" align=left width=44, heigth=33>
<div class="alert alert-warning">
    <b>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b>
    
–¢—É—Ç –µ—Å—Ç—å –µ—â–µ –æ–¥–∏–Ω –º–æ–º–µ–Ω—Ç: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–∏–ª–∞—Å—å —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è. –¢—É—Ç –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤—Å—ë —ç—Ç–æ –≤ —Ü–∏–∫–ª–µ –∏ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è....

–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –≤—Ä—è–¥–ª–∏ –ø–æ–ª—É—á–∏—Ç—Å—è, —Å–ª–∏—à–∫–æ–º –æ–Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞. –ü–æ—ç—Ç–æ–º—É –æ—á–µ–Ω—å —Å–æ–≤–µ—Ç—É—é —Ä–∞–∑–¥–µ–ª–∏—Ç—å –µ—ë –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏  –∏ –ø–æ–¥ –∫–∞–∂–¥—É—é –∑–∞–¥–∞—á—É –Ω–∞–ø–∏—Å–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é. –°–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ç–æ—á–Ω–æ —è –±—ã –æ—Ç–¥–µ–ª–∏–ª –ª–æ–≥–∏–∫—É —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.



</div>

<div class="alert" style="background-color:#ead7f7;color:#8737bf">
    <font size="3"><b>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞</b></font>
   
–ü–æ–Ω—è–ª, –∏—Å–ø—Ä–∞–≤–∏–ª. –§—É–Ω–∫—Ü–∏—é —Ä–∞–∑–±–∏–ª –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏ –ø–µ—Ä–µ–Ω–µ—Å –ø–æ–±–ª–∏–∂–µ –∫ –º–µ—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—è.

</div>

<img src="https://emojigraph.org/media/apple/check-mark-button_2705.png" align=left width=33, heigth=33>
<div class="alert alert-success">
    <b>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞ v2</b>
    
üôè </div>

## 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞

–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏, —Å–º–æ—Ç—Ä–∏–º, —Å —á–µ–º –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è —Ä–∞–±–æ—Ç–∞—Ç—å.


```python
pd.set_option('max_colwidth', None)
pd.options.mode.chained_assignment = None

df = pd.read_csv(r'C:\Users\paul-\Desktop\text_project\toxic_comments.csv').drop('Unnamed: 0', axis=1)

display(df.sample(10, random_state=RANDOM_STATE))
print(df.info())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>toxic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1349</th>
      <td>"\n\n FA article \n\nPlease refrain from repeatedly adding the unsourced assertion that you are the only legal IP holder to Firearms. Not only is it incorrect, it is against our policies on no original research, verifiability, and conflict of interest. You've been warned before about it, further insertion of the material will result in blocks.SWATJester Son of the Defender "</td>
      <td>0</td>
    </tr>
    <tr>
      <th>42630</th>
      <td>"\nI agree with Jaakobu. I replaced the lead because I could not find one article where someone tried to hide another transliteration later in the article, and after reviewing the page history I can only conclude that there is some goal to create a weight in one direction or the other by those seeking the removal of the transliteration. I too would like to see some sourcing for the hand-waving claim that ""it is an arabic word"" over and over again. Just because you repeat a claim over and over does not make it so, that is merely Proof by assertion."</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149949</th>
      <td>Are you sure \nBecause I got it from the Soviet Abkhazia page. I trust you, but that is what was found on that page, I though it would be useful. That is why.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>73013</th>
      <td>Decimal - round numbers \n\nThe decimal section, claiming why decimal was/is so common says (among other reasons):\na human comfort level with rounded figures and the ease it can express and calculate very large numbers\nThis makes no sense.  ANY base will yield round numbers 10000 is 'round' in any base it just means different values in different bases.  And the ability to calculate very large numbers is a function of positional notation, not the paricular base chosen for it.  I've removed those two points. 164.144.252.26</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7396</th>
      <td>"== Your edit to Ghost. ==\n\n[[subst:test1|Ghost}} |  Talk |  "</td>
      <td>0</td>
    </tr>
    <tr>
      <th>156000</th>
      <td>Indonesia's Muslim population looks wrong - 202mil. I doubt there are that many people there.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137917</th>
      <td>I personally don't find so strange the theory about Martians. The people that exist somewhere at the arrival of the settlers can be natives in a relation with them. I don't agree with the use of the term for the articles you enumerated either</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18996</th>
      <td>I'm sorry, I checked Kings Canyon (Utah) and nothing showed up. So I changed it hoping that u've made a mistake by writing (Utah).\nDako1 January 19 2010 1:29 (UTC)</td>
      <td>0</td>
    </tr>
    <tr>
      <th>39593</th>
      <td>PS: And yes, hopefully we can meet some day under better and more pleasent circumstances. Take care</td>
      <td>0</td>
    </tr>
    <tr>
      <th>53274</th>
      <td>Question about guardian angels\nAccording to the wiki article on demonology, there are 133,306,668 demons and the bible says\n1/3 of the angels fell from heaven and became demons.  Therefore, this would mean that there\nare 266,613,336 good angels (i.e. 133,306,668 + 266,613,336 = 399,920,004 total good and bad\nangels with the bad angels being 1/3 of 399,920,004)\n\nGiven that there are about 6 billion people on the earth currently, this would mean that\na guardian angel would have to have more than one person he/she/it is responsible for.\nI assume that God would not assign someone a demon to be his/her guardian angel, so this\nmakes the number of possible guardian angels at maximum, 266,613,336.  Since the world\npopulation is now 6,542,502,827 this would mean that each guardian angel would have to\nbe assigned 24 to 25 people.\n\nDoes the above really sound reasonable to you guys?\n\nAlso, what did those 266 million guardian angels do when there was less than 266 million\npeople on the earth? (The world population reached 266 million around the year 1000 or so)</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>


    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159292 entries, 0 to 159291
    Data columns (total 2 columns):
     #   Column  Non-Null Count   Dtype 
    ---  ------  --------------   ----- 
     0   text    159292 non-null  object
     1   toxic   159292 non-null  int64 
    dtypes: int64(1), object(1)
    memory usage: 2.4+ MB
    None
    

–í—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 159292 —Å—Ç—Ä–æ–∫–∏, –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç. –í —Ç–µ–∫—Å—Ç–∞—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä –∏ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –Ω–∏—Ö; –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞, –∏–∑–±–∞–≤–ª—è—Ç—å—Å—è –æ—Ç —Ç–æ—á–µ–∫, —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–æ–≤ –Ω–µ –±—É–¥–µ–º, –ø–æ—Å–∫–æ–ª—å–∫—É –≤ –ø—Ä–æ–µ–∫—Ç–µ –∑–∞–¥–µ–π—Å—Ç–≤—É–µ–º —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT'–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–∞–º–∞ –æ—Ç–ª–∏—á–Ω–æ —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏ –æ—Ç –ª–∏—à–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –º–æ–∂–µ—Ç '–ø–æ—Ç–µ—Ä—è—Ç—å' –∫–æ–Ω—Ç–µ–∫—Å—Ç.


```python
corpus = df.copy()

corpus['text'] = corpus['text'].str.replace(r'[^a-zA-Z\'.]', ' ', regex=True).str.lower()

display(corpus.sample(10, random_state=RANDOM_STATE))
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>toxic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1349</th>
      <td>fa article   please refrain from repeatedly adding the unsourced assertion that you are the only legal ip holder to firearms. not only is it incorrect  it is against our policies on no original research  verifiability  and conflict of interest. you've been warned before about it  further insertion of the material will result in blocks.swatjester son of the defender</td>
      <td>0</td>
    </tr>
    <tr>
      <th>42630</th>
      <td>i agree with jaakobu. i replaced the lead because i could not find one article where someone tried to hide another transliteration later in the article  and after reviewing the page history i can only conclude that there is some goal to create a weight in one direction or the other by those seeking the removal of the transliteration. i too would like to see some sourcing for the hand waving claim that   it is an arabic word   over and over again. just because you repeat a claim over and over does not make it so  that is merely proof by assertion.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149949</th>
      <td>are you sure  because i got it from the soviet abkhazia page. i trust you  but that is what was found on that page  i though it would be useful. that is why.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>73013</th>
      <td>decimal   round numbers   the decimal section  claiming why decimal was is so common says  among other reasons   a human comfort level with rounded figures and the ease it can express and calculate very large numbers this makes no sense.  any base will yield round numbers       is 'round' in any base it just means different values in different bases.  and the ability to calculate very large numbers is a function of positional notation  not the paricular base chosen for it.  i've removed those two points.    .   .   .</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7396</th>
      <td>your edit to ghost.       subst test  ghost      talk</td>
      <td>0</td>
    </tr>
    <tr>
      <th>156000</th>
      <td>indonesia's muslim population looks wrong      mil. i doubt there are that many people there.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>137917</th>
      <td>i personally don't find so strange the theory about martians. the people that exist somewhere at the arrival of the settlers can be natives in a relation with them. i don't agree with the use of the term for the articles you enumerated either</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18996</th>
      <td>i'm sorry  i checked kings canyon  utah  and nothing showed up. so i changed it hoping that u've made a mistake by writing  utah . dako  january               utc</td>
      <td>0</td>
    </tr>
    <tr>
      <th>39593</th>
      <td>ps  and yes  hopefully we can meet some day under better and more pleasent circumstances. take care</td>
      <td>0</td>
    </tr>
    <tr>
      <th>53274</th>
      <td>question about guardian angels according to the wiki article on demonology  there are             demons and the bible says     of the angels fell from heaven and became demons.  therefore  this would mean that there are             good angels  i.e.                                         total good and bad angels with the bad angels being     of               given that there are about   billion people on the earth currently  this would mean that a guardian angel would have to have more than one person he she it is responsible for. i assume that god would not assign someone a demon to be his her guardian angel  so this makes the number of possible guardian angels at maximum             .  since the world population is now               this would mean that each guardian angel would have to be assigned    to    people.  does the above really sound reasonable to you guys   also  what did those     million guardian angels do when there was less than     million people on the earth   the world population reached     million around the year      or so</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>


–ü—Ä–æ–≤–µ—Ä–∏–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.


```python
print(corpus.duplicated().sum())
```

    236
    

236 –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π. –û–Ω–∏ –º–æ–≥—É—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏ –ø–æ–ø–∞–¥–µ–Ω–∏–∏ –≤ –æ–±–µ –≤—ã–±–æ—Ä–∫–∏, –ø–æ—ç—Ç–æ–º—É –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –Ω–∏—Ö.


```python
corpus = corpus.drop_duplicates()
print(corpus.info())
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 159056 entries, 0 to 159291
    Data columns (total 2 columns):
     #   Column  Non-Null Count   Dtype 
    ---  ------  --------------   ----- 
     0   text    159056 non-null  object
     1   toxic   159056 non-null  int64 
    dtypes: int64(1), object(1)
    memory usage: 3.6+ MB
    None
    

–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ –ø—Ä–∏—Å—Ç—É–ø–∞—Ç—å –∫ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ - –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º—É –æ–±—É—á–µ–Ω–∏—é –∫–ª–∞—Å—Å—Ñ–∏–∫–∞—Ç–æ—Ä–∞.

## 2. –û–±—É—á–µ–Ω–∏–µ

–£—á–∏—Ç—ã–≤–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ–±–∏–ª–∏–µ —Å–ª–µ–Ω–≥–∞ –∏ –Ω–µ–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ç–æ—Ç –∂–µ BERT, –æ–±—É—á–∞–≤—à–∏–π—Å—è –Ω–∞ –≤—ã—Ö–æ–ª–æ—â–µ–Ω–Ω—ã—Ö –∫–Ω–∏–∂–∫–∞—Ö –∏ —Å—Ç–∞—Ç—å—è—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏, –Ω–∞–º –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç. –ú–æ–∂–Ω–æ –±—ã–ª–æ –±—ã '–¥–æ—É—á–∏—Ç—å' –º–æ–¥–µ–ª—å —Å–∞–º–æ–º—É, –æ–¥–Ω–∞–∫–æ —Ä–µ–±—è—Ç–∞ –∏–∑ Unitary –ª—é–±–µ–∑–Ω–æ –ø–æ–¥–µ–ª–∏–ª–∏—Å—å —É–∂–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Ä—É—á–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–º –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ BERT'–æ–º, –ø–æ—ç—Ç–æ–º—É –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–Ω–æ –µ–≥–æ.


```python
tokenizer = transformers.BertTokenizer.from_pretrained('unitary/toxic-bert')

config = transformers.BertConfig.from_pretrained('unitary/toxic-bert')
model = transformers.BertModel.from_pretrained('unitary/toxic-bert', config=config).cuda()
```

–†–µ—Å—É—Ä—Å—ã –Ω–∞—à–µ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ –¥–æ–≤–æ–ª—å–Ω–æ —Å–∫—Ä–æ–º–Ω—ã, –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã —Ü–µ–ª–∏–∫–æ–º, –∏–∑ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –±—ã–ª–æ –±—ã –∑–∞–Ω—è—Ç–∏–µ–º –Ω–µ–±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã–º –∏ —á–µ—Ä–µ—Å—á—É—Ä –≤—Ä–µ–º—è–∑–∞—Ç—Ä–∞—Ç–Ω—ã–º, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –∏ –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω–µ–π. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –Ω–∞–º —Ö–æ—Ç—å –∫–∞–∫-—Ç–æ –º–∞–ª–æ-–º–∞–ª—å—Å–∫–∏ –Ω–∞–¥–æ —É–¥–æ—Å—Ç–æ–≤–µ—Ä–∏—Ç—å—Å—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ç–æ–π –∏–ª–∏ –∏–Ω–æ–π –æ—Ü–µ–Ω–∫–∏. –í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –ø–æ–∂–∞–ª—É–π, –ø–æ–ø—Ä–æ–±—É–µ–º –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫–∏ (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ f1-–º–µ—Ä–∞) –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö, —Å –ø–æ–º–æ—â—å—é –±—É—Ç—Å—Ç—Ä–µ–ø–∞. –û–±—É—á–∏–º 50 –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—ã–±–æ—Ä–∫–∞—Ö –∏–∑ 1000 —Ç–µ–∫—Å—Ç–æ–≤ –∫–∞–∂–¥–∞—è –∏ –ø–æ–ª—É—á–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ–±—ã –∏–∑—É—á–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–∞–∑–±—Ä–æ—Å –∏ —Å–º–µ—â–µ–Ω–∏–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –ø–æ–ª—É—á–∏–º 50 –æ—Ü–µ–Ω–æ–∫, –∏ –≤ –Ω–∞—à–µ–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–∏ –ø–æ—É—á–∞—Å—Ç–≤—É–µ—Ç 50000 —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–ª–∞–≥–∞—é, 50000 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ 50 –æ—Ü–µ–Ω–æ–∫ –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å, —á—Ç–æ–±—ã —Å–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫–æ–µ-–Ω–∏–∫–∞–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–± —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–µ.

–ò–∑-–∑–∞, –æ–ø—è—Ç—å –∂–µ, —á–µ—Ä–µ–∑–º–µ—Ä–Ω–æ–π –≤—Ä–µ–º—è- –∏ —Ä–µ—Å—É—Ä—Å–æ–∑–∞—Ç—Ä–∞—Ç–Ω–æ—Å—Ç–∏ –æ–±—É—á–∏–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –±—É–¥–µ—Ç –ª–∏ –æ–Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å –≥–ª–∞–≤–Ω–æ–º—É —É—Å–ª–æ–≤–∏—é (–∑–Ω–∞—á–µ–Ω–∏–µ f1 > 0.75). –ï—Å–ª–∏ –∂–µ –Ω–µ—Ç, –Ω–∞–π–¥–µ–º –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± –¥–æ–±–∏—Ç—å—Å—è –Ω—É–∂–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.

–ø–æ—Ä—è–¥–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π:
1. —Å–æ–∑–¥–∞–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ 1000 —Ç–µ–∫—Å—Ç–æ–≤
2. —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏
3. –¥–µ–ª–∏–º –≤—ã–±–æ—Ä–∫—É –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
4. –æ–±—É—á–∞–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é
5. –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
6. –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
7. –ø–æ–≤—Ç–æ—Ä—è–µ–º –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω–æ–µ –¥–æ 50 —Ä–∞–∑
8. —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫


```python
def samp_tokenization(data, text_col='text', target_col='target', samp_size=1000):
    
    state = np.random.RandomState(RANDOM_STATE)

    corp_samp = data.sample(n=samp_size, replace=False, random_state=state)

    tokenized = corp_samp[text_col].apply(lambda x: tokenizer.encode(x,
                                                                     add_special_tokens=True,
                                                                     truncation=True,
                                                                     max_length=512
                                                                    )
                                         )

    max_len = 0
    for i in tokenized.values:
        if len(i) > max_len:
            max_len = len(i)

    padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])

    attention_mask = np.where(padded != 0, 1, 0)
    
    return padded, attention_mask, corp_samp
```


```python
def samp_embed_forming(emb_batch_size=100, padded=None, attention_mask=None):
    
    batch_size = emb_batch_size
    embeddings = []

    for i in notebook.tqdm(range(padded.shape[0] // batch_size)):

        batch = torch.tensor(padded[batch_size*i:batch_size*(i+1)], device=device)
        attention_mask_batch = torch.tensor(attention_mask[batch_size*i:batch_size*(i+1)], device=device)

        with torch.no_grad():
            batch_embeddings = model(batch.cuda(), attention_mask=attention_mask_batch)

        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())
    
    return embeddings
```


```python
def sample_feature_forming(data, text_col='text', target_col='target', samp_size=1000, emb_batch_size=100):
    
    padded, attention_mask, corp_samp = samp_tokenization(data, text_col, target_col, samp_size)
    
    embeddings = samp_embed_forming(emb_batch_size, padded, attention_mask)

    features = np.concatenate(embeddings)
    
    return features, corp_samp[target_col]
```


```python
def baseline_scoring(X, y, test_size=0.25):
    
    state = np.random.RandomState(RANDOM_STATE)
    
    lgr = LogisticRegression(random_state=RANDOM_STATE)

    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        test_size=test_size,
                                                        shuffle=True,
                                                        random_state=state
                                                       )

    lgr.fit(X_train, y_train)

    y_pred_train = lgr.predict(X_train)
    y_pred = lgr.predict(X_test)

    train_score = round(f1_score(y_train, y_pred_train), 2)
    test_score = round(f1_score(y_test, y_pred), 2)

    return train_score, test_score
```


```python
def glue_data(data,
              text_col='text',
              target_col='target',
              chunks_num=2,
              samp_size=1000,
              emb_batch_size=100,
              iter_scoring=False,
              test_size=0.25):
    
    if iter_scoring == True:
        train_scores = []
        test_scores = []
    else:
        pass
    
    dcopy = data.copy()
    
    pbar = notebook.tqdm(total=chunks_num, leave=True, desc='total progress')
    
    print('________________ chunk_1 ________________')
    
    X, y = sample_feature_forming(dcopy, text_col, target_col, samp_size, emb_batch_size)
    
    dcopy = dcopy.drop(index=y.index.to_list(), axis=0)
    
    pbar.update()
    
    if iter_scoring == True:
        train_score_0, test_score_0 = baseline_scoring(X, y, test_size=test_size)
        train_scores.append(train_score_0)
        test_scores.append(test_score_0)
    else:
        pass
    
    for i in range(chunks_num - 1):
        
        print('________________ chunk_{} ________________'.format(i + 2))
        
        X_1, y_1 = sample_feature_forming(dcopy, text_col, target_col, samp_size, emb_batch_size)
        
        dcopy = dcopy.drop(index=y_1.index.to_list(), axis=0)
        
        X = np.concatenate((X, X_1))
        y = pd.concat((y, y_1))
        
        pbar.update()
        
        if iter_scoring == True:
            train_score_1, test_score_1 = baseline_scoring(X_1, y_1, test_size=test_size)
            train_scores.append(train_score_1)
            test_scores.append(test_score_1)
        else:
            pass
        
    if iter_scoring == True:
        return X, y, train_scores, test_scores
    else:
        return X, y
```


```python
X, y, train_scores, test_scores = glue_data(corpus,
                                            'text',
                                            'toxic',
                                            chunks_num=50,
                                            samp_size=1000,
                                            emb_batch_size=100,
                                            iter_scoring=True
                                           )
```

–ù–∞ –≤—Å–µ –ø—Ä–æ –≤—Å–µ —É—à–ª–æ –ø–æ—Ä—è–¥–∫–∞ 55 –º–∏–Ω—É—Ç. –ß—Ç–æ –∂, –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—É—á–µ–Ω—ã. –ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π:


```python
fig, ax = plt.subplots(1, 2, figsize=(15, 5))

ax[0].hist(x=train_scores, bins=50, color='silver', alpha=0.85)
ax[0].set_title('train_scores_distribution: Œº={}; std={}; median={}'.format(round(np.mean(train_scores), 3), round(np.std(train_scores), 2), round(np.median(train_scores), 3)))
ax[0].set_xlabel('f1_score')
ax[0].set_ylabel('count')

ax[0].axvline(round(np.mean(train_scores), 3), color='r', linestyle='--')
ax[0].axvline(round(np.median(train_scores), 3), color='b', linestyle='--')

ax[1].hist(x=test_scores, bins=50, color='silver', alpha=0.85)
ax[1].set_title('test_scores_distribution: Œº={}; std={}; median={}'.format(round(np.mean(test_scores), 3), round(np.std(test_scores), 2), round(np.median(test_scores), 3)))
ax[1].set_xlabel('f1_score')
ax[1].set_ylabel('count')

ax[1].axvline(round(np.mean(test_scores), 3), color='r', linestyle='--')
ax[1].axvline(round(np.median(test_scores), 3), color='b', linestyle='--')

fig.tight_layout()
plt.show()
```


    
![png](output_31_0.png)
    


–ß—Ç–æ –≤–∏–¥–∏–º:
1. –ü–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–µ–º–æ–µ —Å–º–µ—â–µ–Ω–∏–µ; –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö –æ–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 1-2% –æ—Ç –µ–¥–∏–Ω–∏—Ü—ã. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Ö–æ—Ä–æ—à–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –∞ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π –¥–æ–ª–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å —Å—É–≥—É–±–æ –ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –≤ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
2. –û–∂–∏–¥–∞–µ–º—ã–π —Ä–∞–∑–±—Ä–æ—Å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 10.2 ¬± 6%, —á—Ç–æ –µ—Å—Ç—å –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∏ –∑–¥–µ—Å—å –µ—Å—Ç—å –∫—É–¥–∞ —Ä–∞—Å—Ç–∏. –í—ã—Å–æ–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞–∑–±—Ä–æ—Å –≤ —Å—Ä–µ–¥–Ω–µ–º –±—É–¥–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è, –∞ –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Ç–∏. –ü—Ä–æ–≤–µ—Ä–∏–º —ç—Ç–æ –¥–∞–ª—å—à–µ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π.
3. –û–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ f1-–º–µ—Ä—ã –ø—Ä–∏ —Ç–µ—Å—Ç–µ (–≤–∞–ª–∏–¥–∞—Ü–∏–∏) = 0.89. –ï—Å—Ç—å –æ–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è, –≥–¥–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ f1 —É–ø–∞–ª–æ –Ω–∏–∂–µ 0.75, –Ω–æ, —Å—É–¥—è –ø–æ –≥—Ä–∞—Ñ–∏–∫—É —ç—Ç–æ –≤—ã–±—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫-—Ç–æ —Å–≤—è–∑–∞–Ω —Å —Ç–µ–º, —á—Ç–æ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–±–æ—Ä–æ–∫ –ø–æ–ø–∞–¥–∞—é—â–∏–µ —Ç—É–¥–∞ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã –∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è, –ø–æ—ç—Ç–æ–º—É –∏–º–µ–Ω–Ω–æ –≤ —ç—Ç—É –≤—ã–±–æ—Ä–∫—É –ø–æ–ø–∞–ª —Å–ª–∞–±–æ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–º–µ–Ω, –∏–ª–∏ —Å —Ç–µ–º, —á—Ç–æ —Ç—É–¥–∞ –ø–æ–ø–∞–ª–∏ –Ω—É —Å–æ–≤—Å–µ–º —É–∂ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –æ–∂–∏–¥–∞–µ–º–∞—è –º–µ—Ç—Ä–∏–∫–∞ –≤—Å–µ –µ—â—ë –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞ –∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—è–º –∑–∞–¥–∞—á–∏. –ï—Å–ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω—ë–º, –¥—É–º–∞—é, –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç –µ—â—ë —Å–∏–ª—å–Ω–µ–µ (–≤–µ—Ä–æ—è—Ç–Ω–æ, –¥–æ ~0.95).

–í—ã–≤–æ–¥:
    **–±—É—Ç—Å—Ç—Ä–µ–ø-–∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ 50000 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –¢–∞–∫–∂–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª—å—é –æ–∫–∞–∂–µ—Ç—Å—è –æ–¥–Ω–∞ –∏–∑ –ª–∏–Ω–µ–π–Ω—ã—Ö. –û–∂–∏–¥–∞–µ–º–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞, –∏ —ç—Ç–æ –ø—Ä–∏ —Ç–æ–º, —á—Ç–æ –º—ã –Ω–µ –¥–µ–ª–∞–ª–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É.**

–î–∞–ª–µ–µ –≤–æ–∑—å–º–µ–º –Ω–∞—à —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –æ–±—É—á–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü–æ–ø—Ä–æ–±—É–µ–º —Ç—Ä–∏ –º–æ–¥–µ–ª–∏: –ª–∏–Ω–µ–π–Ω—ã–µ - —Ö–æ—Ä–æ—à–æ –∑–∞—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–≤—à—É—é —Å–µ–±—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ svm, - –∞ —Ç–∞–∫–∂–µ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π.

–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã –ª–∏ –∫–ª–∞—Å—Å—ã –≤–Ω—É—Ç—Ä–∏ –Ω–∞—à–µ–π –≤—ã–±–æ—Ä–∫–∏:


```python
print(y.value_counts(normalize=True))
```

    0    0.89758
    1    0.10242
    Name: toxic, dtype: float64
    

–ù–∞–±–ª—é–¥–∞–µ–º —Å–∏–ª—å–Ω—É—é –¥–∏—Å–ø—Ä–æ–ø–æ—Ä—Ü–∏—é –∫–ª–∞—Å—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ 9 –∫ 1). –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —ç—Ç–æ –≥—Ä–µ—à–æ–∫ –≤—ã–±–æ—Ä–∫–∏ –∫—Ä–∞–π–Ω–µ –º–∞–ª–∞, –Ω–æ –≤—Å—ë-—Ç–∞–∫–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä–∏–º –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç:


```python
print(corpus['toxic'].value_counts(normalize=True))
```

    0    0.898382
    1    0.101618
    Name: toxic, dtype: float64
    

–ü–æ—Ö–æ–∂–µ, —ç—Ç–æ –±–æ–ª–µ–∑–Ω—å –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä–∫—É –Ω–µ –±—É–¥–µ–º; –ø—Ä–æ—Å—Ç–æ —É–∫–∞–∂–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π 'class_weight'.

–ï—â—ë –æ–¥–∏–Ω –º–æ–º–µ–Ω—Ç. –•–æ—Ç—å –º—ã –∏ —É–¥–∞–ª–∏–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, —É –Ω–∞—Å –≤—Å–µ –µ—â—ë –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ—è–≤–ª–µ–Ω–∏—è –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ. –ü—Ä–æ–≤–µ—Ä–∏–º –∏—Ö –Ω–∞–ª–∏—á–∏–µ:


```python
print(len(X) - len(np.unique(X, axis=0)))
```

    140
    

140 –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫ –≤ –º–∞—Å—Å–∏–≤–µ. –ù–∞–º –Ω—É–∂–Ω–æ –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –Ω–∏—Ö, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –º–æ–≥—É—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏, –ø–æ–ø–∞–≤ –≤ —Ä–∞–∑–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏.


```python
feature_df = pd.DataFrame(data=X)
y_array = np.array(y)
feature_df['toxic'] = y_array

print(feature_df.iloc[:,:-1].duplicated().sum())
feature_df = feature_df.drop_duplicates(subset=feature_df.iloc[:,:-1].columns)
print(feature_df.iloc[:,:-1].duplicated().sum())
```

    140
    0
    

–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ –≥–ª–∞–¥–∫–æ. –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ.


```python
print(feature_df.info())
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 49860 entries, 0 to 49998
    Columns: 769 entries, 0 to toxic
    dtypes: float32(768), int64(1)
    memory usage: 146.8 MB
    None
    

–£ –Ω–∞—Å –æ—Å—Ç–∞–ª–æ—Å—å 49860 —Å—Ç—Ä–æ–∫ –∏–∑ 50000, —Ç–æ –µ—Å—Ç—å 99.72% –¥–∞–Ω–Ω—ã—Ö. –î—É–º–∞—é, –Ω–∞–º –¥–æ–ª–∂–Ω–æ —ç—Ç–æ–≥–æ —Ö–≤–∞—Ç–∏—Ç—å, —Ç–∞–∫ —á—Ç–æ –ø—Ä–∏—Å—Ç—É–ø–∏–º –∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –Ω–∞ –≤—ã–±–æ—Ä–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π.


```python
X_train, X_test, y_train, y_test = train_test_split(feature_df.drop('toxic', axis=1),
                                                    feature_df['toxic'],
                                                    test_size=test_size,
                                                    shuffle=True,
                                                    stratify=feature_df['toxic'],
                                                    random_state=RANDOM_STATE
                                                   )
```

–í—Å–µ –≥–æ—Ç–æ–≤–æ, –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏—Å—Ç—É–ø–∞–µ–º –∫ –ø–æ–∏—Å–∫—É –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏. –û–ø—Ä–µ–¥–µ–ª–∏–º —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è GridSearchCV.


```python
pipe_model = Pipeline(
    [
        ('model', LogisticRegression(random_state=RANDOM_STATE))
    ]
)

pdists = [
    {
        'model': [DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')],
        'model__max_depth': [None] + [i for i in range(1, 7)],
        'model__max_features': range(1, 20)
    },
    {
        'model': [LogisticRegression(solver='liblinear',
                                     penalty='l2',
                                     max_iter=1000,
                                     class_weight='balanced',
                                     random_state=RANDOM_STATE
                                    )
               ],
        'model__C': [round(i, 2) for i in np.logspace(-2, 5, 15, base=2)]
    },
    {
        'model': [LinearSVC(class_weight='balanced', random_state=RANDOM_STATE)],
        'model__C': [round(i, 2) for i in np.logspace(-2, 6, 5, base=2)]
    }
]
```

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º grid-–æ–±—ä–µ–∫—Ç.


```python
grid = GridSearchCV(pipe_model,
                    param_grid=pdists,
                    cv=3,
                    scoring='f1',
                    n_jobs=1,
                    verbose=10
                   )
```

–ù–∞–π–¥–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å.


```python
grid_start = time.time()

grid.fit(X_train, y_train)

grid_search_time = time.time() - grid_start

print(f'–í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–æ {round((grid_search_time / 60), 2)} –º–∏–Ω—É—Ç')
```

–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞. –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ cv-f1 –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.


```python
print(grid.best_estimator_)
print(grid.best_score_)
```

    Pipeline(steps=[('model',
                     LogisticRegression(C=0.71, class_weight='balanced',
                                        max_iter=1000, random_state=17,
                                        solver='liblinear'))])
    0.9081574472833279
    

–ü–æ—Ö–æ–∂–µ, –∫–∞–∫ –º—ã –∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–∏, –ø–æ–±–µ–¥–∏–ª–∞ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è. cv-f1 –ø–æ—á—Ç–∏ —Ç—é—Ç–µ–ª—å–∫–∞ –≤ —Ç—é—Ç–µ–ª—å–∫—É —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ–∂–∏–¥–∞–µ–º–æ–π. –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É.


```python
print(f1_score(y_test, grid.best_estimator_.predict(X_test)))
```

    0.8958257713248639
    

–¢–µ—Å—Ç–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ —á—É—Ç—å —Ö—É–∂–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç ~0.896, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–∂–∏–¥–∞–µ–º–æ–≥–æ (0.91 ¬± 0.04). –ù–∞—à–∞ –º–æ–¥–µ–ª—å –Ω–µ —Ç–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –Ω–æ –∏ –¥–µ–ª–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –ü–æ–ª–∞–≥–∞—é, –∑–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞, –∏ –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ, –∞ –º—ã –º–æ–∂–µ–º –±—ã—Ç—å –≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ —É–≤–µ—Ä–µ–Ω—ã –≤ —Ç–æ–º, —á—Ç–æ –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–Ω–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Ö–æ—Ä–æ—à–æ.

## 3. –í—ã–≤–æ–¥—ã

**–ü–æ –ø–æ—Ä—è–¥–∫—É:**
1. –•–æ—Ä–æ—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å - —ç—Ç–æ, –∫–∞–∫ —è –ø–æ–Ω–∏–º–∞—é, –ø–æ–ª–¥–µ–ª–∞ –≤ –ø–æ–¥–æ–±–Ω–æ–π –∑–∞–¥–∞—á–µ, –∞ —Ç–æ –∏ –±–æ–ª—å—à–µ. –£ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–∏—Ö –∫–æ–ª–ª–µ–≥, –ø—ã—Ç–∞–≤—à–∏—Ö—Å—è —Ä–µ—à–∞—Ç—å –µ—ë —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ BERT'–∞, –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–¥–Ω—è—Ç—å –º–µ—Ç—Ä–∏–∫—É f1 –≤—ã—à–µ 0.7 ¬± 5. –û–≥—Ä–æ–º–Ω–∞—è –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å Unitary –∑–∞ '—Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ' BERT'–∞.
2. –õ–æ–≥—Ä–µ–≥-–∞–ª–≥–æ—Ä–∏—Ç–º —Ö–æ—Ä–æ—à–æ —Å–µ–±—è –∑–∞—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª. –û–Ω —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫ –∏–º–µ—é—â–∏–º—Å—è –¥–∞–Ω–Ω—ã–º.
3. –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º —Å –æ–∂–∏–¥–∞–µ–º—ã–º.
4. –ú—ã –º–æ–∂–µ–º –±—ã—Ç—å —É–≤–µ—Ä–µ–Ω—ã –≤ —Ç–æ–º, —á—Ç–æ —Å –≤—ã—Å–æ–∫–æ–π –¥–æ–ª–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å —Ç–æ–∂–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ, –æ–¥–Ω–∞–∫–æ –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ —Ä–∞–±–æ—Ç–µ —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –µ—ë –Ω–∞ –±–æ–ª—å—à–µ–º –æ–±—ä–µ–º–µ –¥–∞–Ω–Ω—ã—Ö.
5. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ, –∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å –ª–∏—Ö–≤–æ–π —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—è–º –∑–∞–¥–∞—á–∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∑–∞–¥–∞—á—É –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π, –∞ –º–æ–¥–µ–ª—å –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ —Ä–∞–±–æ—Ç—É.
6. –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –¥–æ–æ–±—É—á–∏—Ç—å –∏–º–µ—é—â—É—é—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω—É–∂–¥ –∑–∞–∫–∞–∑—á–∏–∫–∞ –∏ —Ä–∞–±–æ—á–µ–π –¥–∏–Ω–∞–º–∏–∫–∏ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã, –≤ —Ç–æ–º —á–∏—Å–ª–µ –∑–∞–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ —Ç–æ–≤–∞—Ä–∞–º –º–∞–≥–∞–∑–∏–Ω–∞.
